# Hypothesis Tests


Up to now, we have discussed the properties of estimators that allow us to sometimes characterize their distributions in finite and large samples. These properties might allow us to say that, for example, our estimated difference in means is equal to a true average treatment effect on average across repeated samples or that it will converge to the true value in large samples. These properties, however, are properties of repeated samples whereas we as researchers will only have access to a single sample. **Statistical inference** is the process of using our single sample to learn about population parameters. There are several ways to do inference that are connected, but perhaps the most ubiquitous in the sciences is the hypothesis test, which is a kind of statistical thought experiment.  




## The lady tasting tea

The lady tasting tea is an example of the core ideas behind hypothesis testing due to R.A. Fisher.[^1] Fisher had prepared a cup of tea for his colleague, the algologist Muriel Bristol. Knowing that she preferred milk in her tea, he poured milk into a tea cup and then poured the hot tea into the milk. Bristol rejected the cup, stating that she preferred the tea to be poured first, then milk. Fisher was apparently incredulous at the idea anyone could tell the difference between a cup poured milk-first or tea-first. So he and another colleague, William Roach, devised a test to see if Bristol had the ability to distinguish the two preparation methods. 

For this test, Fisher and Roach prepared 8 cups of tea, 4 prepared milk-first and 4 prepared tea-first. They then presented the cups to Bristol in a random order (though she knew there were 4 of each type), and she proceeded to identify all of the cups correctly. At a first glance, this seems like good evidence that she can tell the difference between the two types, but a skeptic like Fisher raised the question: "could she have just been randomly guessing and got lucky?" This led Fisher to a **statistical thought experiment**: what would the probability of guessing all cups correctly *if* she were guessing randomly?

To calculate the probability of Bristol's achievement, we can note that "randomly guessing" here would mean that she were selecting a group of 4 cups to be labeled milk-first from the 8 cups available. Using basic combinatorics, there are 70 ways to choose 4 cups among 8, but only 1 of those arrangements would be correct. Thus, if randomly guessing means choosing among those 70 options with equal chance, then the probability of guessing correctly is 1/70 or $\approx 0.014$. This probability being so low implies that the hypothesis of random guessing may be implausible. 

The story of the lady tasting tea encapsulates many of the core elements of hypothesis testing. Hypothesis testing is about taking our observed estimate (Bristol guessing all the cups correctly) and seeing how likely that observed estimate would be under some assumption or hypothesis about the data generating process (Bristol was randomly guessing). When the observed estimate is very unlikely under the maintained hypothesis, we might view this as evidence against that hypothesis. Thus, hypothesis tests help us assess evidence for particular guesses about the DGP. 


[^1]: The analysis here largely comes from @Senn12. 


::: {.callout-note}

## Notation alert

For the rest of this chapter, we'll introduce the concepts following the notation in the past chapters. We'll usually assume that we have a random (iid) sample of random variables $X_1, \ldots, X_n$ from a distribution, $F$. We'll focus on estimating some parameter, $\theta$ of this distribution (like the mean, median, variance, etc). We'll refer to $\Theta$ as the set of possible values of $\theta$, or the **parameter space**.

:::

## Hypotheses

In the context of hypothesis testing, hypotheses are just statements about the population distribution. In particular, we will make statements that $\theta = \theta_0$ where $\theta_0 \in \Theta$ is the hypothesized value of $\theta$. Hypotheses are ubiquitous in empirical work, but here are some examples to give you a flavor:

- The population proportion of US citizens that identify as Democrats is 0.33. 
- The population difference in average voter turnout between households who received receiving get-out-the-vote mailers vs those who did not is 0. 
- The difference in average incidence of human rights abuse in countries that signed a human rights treaty vs those countries that did not sign is 0. 

Each of these is a statement about the true DGP. The latter two are very common: when $\theta$ represents the difference in means between two groups, then $\theta = 0$ is the hypothesis of no true difference in population means, or no treatment effect (if the causal effect is identified). 

The goal of hypothesis testing is to adjudicate between two complementary hypotheses. 

::: {#def-null}

The two hypotheses in a hypothesis test are called the **null hypothesis** and the **alternative hypothesis**, denoted as $H_0$ and $H_1$, respectively. 

:::

These hypotheses are complementary, so that if the null hypothesis $H_0: \theta \in \Theta_0$, then the alternative hypothesis is $H_1: \theta \in \Theta_0^c$. The "null" in null hypothesis might seem odd until you realize that most hypotheses being tested are that there is no effect of some treatment or no difference in means. For example, suppose $\theta$ is the difference in mean support for expanding legal immigration between a treatment group that received a pro-immigrant message along with some facts about immigration and a control group that just received the factual information. Then, the typical null hypothesis would be no difference in means or $H_0: \theta = 0$ and the alternative would be $H_1: \theta \neq 0$. 

There are two types of tests that differ by the form of their null and alternative hypotheses. A **two-sided test** is of the form
$$
H_0: \theta = \theta_0 \quad\text{versus}\quad H_1: \theta \neq \theta_0,
$$
where the "two-sided" part refers to how the alternative contains values of $\theta$ above and below the null value $\theta_0$. A **one-sided test** has the form
$$
H_0: \theta \leq \theta_0 \quad\text{versus}\quad H_1: \theta > \theta_0,
$$
or
$$
H_0: \theta \geq \theta_0 \quad\text{versus}\quad H_1: \theta < \theta_0.
$$
Two-sided tests are much more common in the social science where we want to know if there is any evidence, positive or negative, against the presumption of no treatment effect or no relationship between two variables. One-sided tests are for situations where we only want evidence in one direction, which is rarely relevant to social science research. One-sided tests also have the downside of being misused to inflate the strength of evidence against the null and so should be avoided. Unfortunately, the math of two-sided tests are also more complicated. 

## The procedure of hypothesis testing

At the most basic level, a **hypothesis test** is a rule that specifies values of the sample data for which we will decide to **reject** the null hypothesis. Let $\mathcal{X}_n$ be the range of the sample---that is, all possible vectors $(x_1, \ldots, x_n)$ that have positive probability of occurring. Then, a hypothesis test describes a region of this space, $R \subset \mathcal{X}_n$, called the **rejection region** where when $(X_1, \ldots, X_n) \in R$ we will **reject** $H_0$ and when the data is outside this region, $(X_1, \ldots, X_n) \notin R$ we **retain**, **accept**, or **fail to reject** the null hypothesis.[^2]

[^2]: Different people and different textbooks describe what to do when do not reject the null hypothesis in different ways. The terminology is not so important so long as you understand that rejecting the null does not mean the null is logically false and "accepting" the null does not mean the null is logically true. These are simply statements about the decision that needs to be made as part of the hypothesis test. 




How do we decide what the rejection region should be? Even though we define the rejection region in terms of the **sample space**, $\mathcal{X}_n$, it's unwieldy to work with the entire vector of data. Instead, we often formulate the rejection region in terms of a **test statistic**, $T = T(X_1, \ldots, X_n)$, where the rejection region becomes
$$
R = \left\{(x_1, \ldots, x_n) : T(x_1, \ldots, x_n) > c\right\},
$$
where $c$ is called the **critical value**. In words, this says that the rejection region are the parts of the sample space that make the test statistic sufficiently large. We reject null hypotheses when the observed data is incompatible with those hypotheses, where the test statistic should be a measure of this incompatibility. Note that the test statistic is a random variable and has a distribution---we will exploit this to understand the different properties of a hypothesis test. 



::: {#exm-biden}

Suppose that $(X_1, \ldots, X_n)$ measure whether a sample of US citizens support the current US president ($X_i = 1$) or not ($X_i = 0$). We might be interested in the test of the null hypothesis that the president does not have the support of a majority of American citizens. Let $\mu = \E[X_i] = \P(X_i = 1)$. Then, a one-sided test would compare the two hypotheses
$$ 
H_0: \mu \leq 0.5 \quad\text{versus}\quad H_1: \mu > 0.5.
$$
In this case, we might use the sample mean as the test statistic, so that $T(X_1, \ldots, X_n) = \Xbar_n$ and we have to find some threshold above 0.5 such that we would reject the null, 
$$ 
R = \left\{(x_1, \ldots, x_n): \Xbar_n > c\right\}.
$$
In words, how much support should we see for the current president before we decide to reject the notion that they lack majority support. Below we are going to select the critical value, $c$, to have nice statistical properties. 
:::

The structure of a reject region will depend on whether a test is one- or two-sided. One-sided tests will take the form $T > c$, whereas two-sided tests will take the form $|T| > c$, since we want to count deviations from either side of the null hypothesis as evidence against that null. 

|              | $H_0$ True   | $H_0$ False   |
|--------------|--------------|---------------|
| Retain $H_0$ | Awesome      | Type II error |
| Reject $H_0$ | Type I error | Great         |

: Typology of testing errors {#tbl-errors}

If we are making a decision about whether to reject a null hypothesis or not, it is possible that we will make the incorrect decision. In particular, there are two ways for us to make errors and two ways for us to be correct in this setting, as shown in @tbl-errors. The labels are confusing, but it's helpful to remember that **type I errors** (said "type one") are labelled so because they are the worse of the two types of errors. This is when we reject a null (say there is a true treatment effect or relationship) when in fact the null is true (there is no true treatment effect or relationship). Type I errors are what we see in the replication crisis: lots of "significant" effects that turn out later to be null. **Type II errors** (said "type two") are considered less problematic: there is a true relationship, but we cannot detect it with our test (cannot reject the null). 

Ideally, we would minimize the chances that we make either a type I or type II error. Unfortunately, because the test statistic is a random variable,  we cannot remove the probability of an error completely. Instead, we will try to derive tests that have some guaranteed performance in terms of minimizing the probability of type I error. To derive this, we can define the **power function** of a test,
$$ 
\pi(\theta) = \P\left(  \text{Reject } H_0 \mid \theta \right) = \P\left( T \in R \mid \theta \right),
$$
which is the probability of rejection as a function of the parameter of interest, $\theta$. This tells us, for example, how likely we are to reject the null of no treatment effect as we vary the true size of the treatment effect. 

From the power function we can define the probability of type I error. 

::: {#def-size}
The **size** of a hypothesis test with the null hypothesis $H_0: \theta = \theta_0$ is 
$$ 
\pi(\theta_0) = \P\left( \text{Reject } H_0 \mid \theta_0 \right).
$$
:::



```{r}
#| label: fig-size-power
#| echo: false
par(mfrow = c(1,2))
curve(dnorm(x), from = -4, to = 4, ylim = c(0, 0.5), bty = "n", las = 1, xlab = "T under the null hypothesis", ylab = "", xaxt = "n", yaxt = "n", col = "black", lwd = 2, main = "Size")
axis(side = 1, at = c(-10, 0, 2.32, 10), labels = c("b", expression(theta[0]), expression(c), "d"),
     line = 0)
polygon(c(2.32,seq(2.32,4,0.01),4), c(0,dnorm(seq(2.32,4,0.01)), 0), col = adjustcolor("grey50", alpha = 0.5), border = NA)
## text(x = 3, y = 0.02, "0.05", pos = 3)
abline(v = 2.32, lty = 2, col = "grey60")
text(x = 0, y = 0.45, "Retain")
text(x = 2.32, y = 0.45, "Reject", pos = 4)
text(x = -1, y = dnorm(-1), expression(P(T[n] ~~ "|" ~~ theta[0])), pos = 2)
curve(dnorm(x), from = -4, to = 4, ylim = c(0, 0.5), bty = "n", las = 1, xlab = "T under an alternative", ylab = "", xaxt = "n", yaxt = "n", col = "grey80", lwd = 2, main = "Power")
axis(side = 1, at = c(-10, 0, 1, 2.32, 10), labels = c("b", expression(theta[0]), expression(theta[1]), expression(c), "d"),
     line = 0)
curve(dnorm(x, mean = 1), from = -4, to = 4, col = "indianred", lwd = 2, add = TRUE)
## text(x = 3, y = 0.02, "0.05", pos = 3)
abline(v = 2.32, lty = 2, col = "grey60")
text(x = 0, y = 0.45, "Retain")
text(x = 2.32, y = 0.45, "Reject", pos = 4)
text(x = -1, y = dnorm(-1), expression(P(T[n] ~~ "|" ~~ theta[0])), pos = 2)
text(x = 2, y = dnorm(2, mean = 1), expression(P(T[n] ~~ "|" ~~ theta[1])), pos = 4, col = "indianred")
polygon(c(2.32,seq(2.32,4,0.01),4), c(0,dnorm(seq(2.32,4,0.01), mean = 1), 0), col = adjustcolor("indianred", alpha = 0.5), border = NA)
```


