# Design-based Inference


## Introduction

Quantitative analysis of social data has an alluring exactness. We can estimate the average minutes of YouTube videos watched down to the millisecond, giving us the aura of true scientists. But the great advantage of quantitative is not the ability to derive cold, hard, three-decimal point estimates. Rather, quantitative methods shine because they allow us to communicate the goals, assumptions, methods, and results of an analysis in a (hopefully) common, compact, and precise mathematical language. This language helps to clarify *exactly* what  you are doing with your data and why you are doing it. 

Unfortunately, this dewy view of quantitative methods is at odds with how these methods are used in the real world. Too often, we find some arbitrary data, apply a statistical tool we are familiar with, and shoehorn the results into a theoretical story that may have a tenuous connection into those first two steps. In this case, quantitative methods provide us with a very specific answer to a murky question about a shapeless target. 

This book is a guide to a better foundation for quantitative analysis, and in particular, statistical inference. Inference is the task of using data that we have to learn something about data we not see. The organizing motto of this book is:

> Precise in stating our goals, transparent in stating our assumptions, and honest in evaluating our results. 

The goals here are the target of our inference: what do we want to learn about who? 


We will focus on a general workflow for statistical inference that will maximize the clarity and usefulness of the techniques in this book. The workflow boils down to answering a series of question about the goals, assumptions, and methods of our analysis. These are the core questions of any quantitative research endeavor:

1. **Population**: who or what do we want to learn about?
2. **Estimand**: what do we want to learn about the population?  
3. **Design/model**: how will we collect the data, or, what assumptions are we making about how the data came to be?
4. **Estimator**: how will we use the data to produce an estimate? 
5. **Uncertainty**: how will we estimate and convey the error associated with my estimate?

A mixture of substantive interests, feasibility, and statistical theory will determine the answer to these questions. They also vary is how specific the answers will be. The choice of population can vary greatly from study to study, whereas there may be many studies that employ the estimand and estimator. 

The third core question highlights a core division in how we approach statistical inference: **design-based inference** vs **model-based inference**. Design-based inference typically focuses on situations in which we have precise knowledge of how our sample was randomly selected from the population. In this framework, uncertainty comes exclusively from the random nature of who is included in the sample. In the **model-based** framework, we treat our data as random variables and propose a probabilistic model for how the data came to be. These models vary in how strong the assumptions are on the model. 

Design-based inference is the framework that most clearly delineates the core inferential questions, and so we will focus on it in this chapter. Its main disadvantages are that it is considerably less general than the model-based approach and that the mathematics of the framework are slightly more complicated. 


## Question 1: Population

Inference is the task of using data that we have to learn facts about the world. The most straightforward setting for this task is when we have a fixed set of units that we want to learn something about, which we call the **population** or **target population**. We are going to focus on random sampling from this population, but to do so, we need to have a a list of units from the population. This list of $N$ units is called the **frame** or **sampling frame**,  and we will index these units by $i \in \mathcal{U} = \{1, \ldots, N\}$. Here we assume that $N$, the size of the population, is known, though this may not be true in all settings. 

The sampling frame is often differs from the target population for feasibility reasons. For example, a target population might be all the households in a given city, but a sampling frame might be the list of all residential telephone numbers for that city. Of course, there are many households that do not have landline telephones and rely on mobile phone exclusively. This gap between the target population and the sampling frame is called **undercoverage** or **coverage bias**. 

::: {#exm-frame-bias}

An early example of frame bias in survey sampling was the *Literary Digest* poll of the 1936 U.S. presidential election. The magazine sent over 10 million ballots to addresses found in automobile registration lists and telephone books. The magazine's poll estimated that Alf Landon, the Republican candidate, would receive 55% of the vote, while the incumbent, President Franklin D. Roosevelt, would only win 41% of the vote. The poll had an impressive sample size of over 2 million respondents. Unfortunately for the *Literary Digest*, Landon only received 37% of the vote. 

Many reasons for this massive polling error have been proposed, including the sampling frame being different from the target population. Only those with either a car or a telephone were included in the sampling frame, and citizens without either overwhelmingly supported Roosevelt. While this is no the only source of bias in this case (differential nonresponse seems to be a particularly big problem), the frame bias does contribute a large part of the error. For more about this poll, see @Squire88.  

:::

One advantage of design-based inference is the how precisely we must articulate the sampling frame. We can be extremely clear about the group of units we are trying to learn about. We shall see that in model-based inference the concept of the population and sampling frame become more amorphous.  


## Question 2: Quantity of Interest

The **quantity of interest** is a numerical summary of the population that we would like to learn about. These quantities are also called **estimands** (roughly the Latin for "the thing to be estimated"). Let $x_1, x_2, \ldots, x_N$ be a fixed set of characteristics, or items, about the population. Drawing on the statistician's favorite home decor, we might think about our population as a set of marbles in a jar and the $x_i$ values indicate, for example, the color of the $i$-th marble. In the survey context, $x_i$ might represent the age or ideology or income of the $i$-th person in the population. 

We can easily define many quantities of interest based on the population characteristics. Generally, these quantities are summaries of the values $x_1, \ldots, x_N$ One of the most common and useful is the **population mean**, which we can define as
$$
\overline{x} = \frac{1}{N} \sum_{i=1}^N x_i.
$$
The population mean is fixed because $N$ and the population characteristics $x_1, \ldots, x_N$ are fixed. One other common estimand in the survey sampling literature is the population total,
$$
t = \sum_{i=1}^N x_i = N\overline{x}.
$$

::: {#exm-subpopulation}

## Subpopulation means

We might also be interested in quantities for different subdomains. Suppose we are interested in estimating the fraction of (say) conservative-identifying respondents support increasing legal immigration limits. Let $d= 1, \ldots, D$ be the number of subdomains or subpopulations. In this case, we might have $d = 1$ are liberal identifiers, $d = 2$ are moderate identifiers, and $d = 3$ are conservative identifiers. We will refer to the subpopulation for each of these groups as $\mathcal{U}_d \subset \{1,\ldots, N\}$ and we define the size of these groups as $N_d = |\mathcal{U}_d$. So, we have $N_3$ would be the number of conservative-identifying citizens in the population. 

The mean for each group is then
$$
\overline{x}_d = \frac{1}{N_d} \sum_{i \in \mathcal{U}_d} x_i.
$$

Subpopulation estimation can be slightly more complicated than population estimation because we may not know who is in which subpopulation until we we actually sample the population. For example, our sampling frame probably doesn't include information about the ideological affiliations of potential respondents. Thus, $N_d$ will be unknown to the researcher, unlike $N$ for the population mean. 

:::

There are many other quantities of interest that we might be interested in, but design-based inference is largely focused on these types of population and subpopulation means and totals. 


## Question 3: Sampling design

Now that we have a clearly defined population, sampling frame, and quantity of interest, we can think about how we might select a sample from our population. We will focus on **probabilistic samples**, where we select units by chance and each unit in the sampling frame has a non-0 probability of being included included in the sample. Let $\mathcal{S} \subset \mathcal{U}$ be a sample and let $\mb{Z} = (Z_1, Z_2, \ldots, Z_N)$ to be a vector of inclusion indicators such that $Z_i = 1$ if $i \in \mathcal{S}$ and $Z_i = 0$ otherwise. We denote these indicators as upper-case letters because they are random variables. We assume the sample size is $|\mathcal{S}| = n$. 


Suppose our sampling frame was Hobbits that were members of the fellowship of the ring. This is a valid, albeit small and fictional population with $\mathcal{U} =$ \{Frodo, Sam, Pip, Merry\}. We can list all six possible samples of size 2 from this population in terms of the sample members $\mathcal{S}$ or, equivalently, the inclusion indicators $\mb{Z}$: 

- $\mathcal{S}_1 =$ \{Frodo, Sam\} with $\mb{Z}_{1} = (1, 1, 0, 0)$
- $\mathcal{S}_2 =$ \{Frodo, Pip\} with $\mb{Z}_{2} = (1, 0, 1, 0)$
- $\mathcal{S}_3 =$ \{Frodo, Merry\} with $\mb{Z}_{3} = (1, 0, 0, 1)$
- $\mathcal{S}_4 =$ \{Sam, Pip\} with $\mb{Z}_{4} = (0, 1, 1, 0)$
- $\mathcal{S}_5 =$ \{Sam, Merry\} with $\mb{Z}_{5} = (0, 1, 0, 1)$
- $\mathcal{S}_6 =$ \{Pip, Merry\} with $\mb{Z}_{6} = (0, 0, 1, 1)$

A **sampling design** is a complete specification of how likely each of these samples is to be selected. That is, we need to determine a selection probability $\pi_j$ for each sample $\mathcal{S}_j$. One of the most widely-studied designs is one that places equal probability on each of the possible samples of size $n$. 

::: {#def-srs}
A **simple random sample** (srs) is a probability sampling design where each possible sample of size $n$ has the same probability 
of occurring. More specifically, let $\mb{z} = (z_{1}, \ldots, z_{N})$ be a particular possible sampling, then, 
$$
\P(\mb{Z} = \mb{z}) = \begin{cases}
{N \choose n}^{-1} &\text{if } \sum_{i=1}^N z_i = n,\\
0 & \text{otherwise}
\end{cases}
$$
:::


In the case of sampling two hobbits, the srs would place $1/{4\choose 2} = 1/6$ probability of each of the above samples $\mathcal{S}_j$. Notice that the simple random sample gives zero probability to any sample does not have exactly $n$ units in the sample. 

Another common sampling design, called the **Bernoulli sampling** chooses each unit independently with the sample probability. 

::: {#def-srs}
**Bernoulli sampling** is a probability sampling design where independent Bernoulli trials with probability of success $q$ determine whether each unit in the population will be included in the sample. More specifically, let $\mb{z} = (z_{1}, \ldots, z_{N})$ be a particular possible sampling. Then, Bernoulli sampling is 
$$
\P(\mb{Z} = \mb{z}) = \P(Z_1 = z_1) \cdots \P(Z_N = z_N) = \prod_{i=1}^N q_i^{Z_i}(1 - q_i)^{1-Z_i}
$$
:::

Bernoulli sampling is simple and the independence of inclusion across units simplifies many calculation. However, the "coin flipping" approach means that the sample size, $N_s = \sum_{i=1}^N Z_i$ will be itself a random variable because it is the result of how many of the coin flips land on "heads." 


## Question 4: Estimator


Now that we have a sampling design and a quantity of interest, we can consider how we might learn about the quantity of interest from our sample. An **estimator** is a function of the sample measurements inteded as a best guess about our quantity of interest. 

If the most common estimand is the population mean, the most popular estimator is the **sample mean**,
$$
\overline{X}_n = \frac{1}{n} \sum_{i=1}^{N}Z_ix_i
$$


The sample mean is a **random** quantity since it varies from sample to sample and those samples are chosen probabilistically. For example, say we have the following measurements of heights from our very small population of hobbits. 


| Unit ($i$) | Height in cm ($x_i$) |
|:-----------|:---------------------|
| 1 (Frodo)  | 124                  |
| 2 (Sam)    | 127                  |
| 3 (Pip)    | 123                  |
| 4 (Merry)  | 127                  |

If we consider a simple random sample of size $n=2$ from this population, we can list out the probability of all possible sample means associated with this sampling design.

| Sample ($j$)     | Probability ($\pi_j$) | Sample mean ($\overline{X}_n$) |
|:-----------------|:----------------------|:-------------------------------|
| 1 (Frodo, Sam)   | 1/6                   | (124 + 127) / 2 = 125.5        |
| 2 (Frodo, Pip)   | 1/6                   | (124 + 123) / 2 = 123.5        |
| 3 (Frodo, Merry) | 1/6                   | (124 + 127) / 2 = 125.5        |
| 4 (Sam, Pip)     | 1/6                   | (127 + 123) / 2 = 125          |
| 5 (Sam, Merry)   | 1/6                   | (127 + 127) / 2 = 127          |
| 6 (Pip, Merry)   | 1/6                   | (123 + 127) / 2 = 125          |


We can simplify this a bit to find the sampling distribution of the sample mean of hobbit height under a simple random sample of size 2:

| Sample mean | Probability |
|-------------|-------------|
| 123.5       | 1/6         |
| 125         | 1/3         |
| 125.5       | 1/3         |
| 127         | 1/6         |


Thus, the sampling distribution combines the population with a sampling design to tell us what values of an estimator are more or less likely. 

::: {.callout-note}

Notice that the sampling distribution of an estimator will depend on the sampling design. Here, we used the simple random sample, but Bernoulli sampling would have produced a different distribution. For example, under Bernoulli sampling, we could end up with a sample of just Frodo, in which case the sample mean would be 124, a value that was not possible with simple random sampling of size $n=2$. 

:::



### Properties of the sampling distribution of an estimator 

Generally speaking, we want "good" estimators, but what makes a "good" estimator? The best estimator would be right all of the time ($\Xbar_n = \overline{x}$ with probability 1), but this is only possible if we conduct a census (sample everyone in the population) or the population does not vary, neither of which are typical situations for the working researcher. 

We instead focus on properties of the sampling distribution of an estimator. We might ask the following types of questions:

- Is the variation in the realizations of the estimator centered on the true value of the quantity of interest? (unbiasedness)
- Is there a lot or a little variation in the realizations of the estimator across different samples from the population? (sample variance)
- On average, how close to the truth is the estimator? (mean square error)

The answers to these questions will depend on (a) the estimator and (b) the sampling design. 


The sampling distribution shows us all the possible values of an estimator across different samples from the population. If we want to summarize this distribution with a single number, we would focus on its expectation, which is a measure of central tendency of the distribution. Roughly speaking, we would like the center of the distribution to be close to and, ideally, equal to the true quantity of interest. If this isn't the case, it means the estimator systematically over- or under-estimates the truth. We call this difference the **bias** of an estimator, and we write it mathematically as 
$$
\textsf{bias}[\Xbar_{n}] = \E[\Xbar_{n}] - \overline{x}.
$$
Any estimator that has bias equal to zero is call an **unbiased** estimator. 

We can calculate the bias of our hobbit srs by first calculating the expected value of the estimator,
$$
\E[\Xbar_{n}] = \frac{1}{6}\cdot 123.5 + \frac{1}{3} \cdot 125 + \frac{1}{3} \cdot 125.5 + \frac{1}{6} \cdot 127 = 125.25,
$$
and compare this to the population mean,
$$
\overline{x} = \frac{1}{4}\left(124 + 127 + 123 + 127\right) = 125.25.
$$
Thus, the sample mean in this simple random sample is unbiased. 


::: {.callout-warning}

We often use the word "bias" to refer to other ways that our research could be systematically incorrect. For example, we might complain that a survey question is biased if it presents a question in leading or misleading manner or if it mismeasures the concept of interest. For example, suppose we wanted to estimate what proportion of a population regularly donates money to a political campaign, but $x_i$ actually measures whether or not they donated on the day of the survey. In this case, $\overline{x}$ would be quite a bit lower than the quantity of interest that we actually want to measure. Textbooks often refer to this gap---between the measures we obtain and the measures we want---as **measurement bias**. This is distinct from the bias of the sample mean. If we took a srs from the population, $\Xbar_{n}$ would still be an unbiased for $\overline{x}$, even if that is the wrong quantity of interest.  

:::

Is the unbiasedness of our hobbit sampling unique? Thankfully, no. We can actually prove that the sample mean will be unbiased for the population mean under a simple random sample. Simply plugging in the definition of the sample mean, we obtain.
$$ 
\E[\Xbar_{n}] = \E\left[\frac{1}{n} \sum_{i=1}^{N} Z_{i}x_{i}\right] = \frac{1}{n} \sum_{i=1}^{N} \E[Z_{i}]x_{i} = \frac{1}{n} \sum_{i=1}^{N} \frac{n}{N}x_{i} = \frac{1}{N} \sum_{i=1}^{N}x_{i} = \overline{x} 
$$
The second equality is the key, where we use $\E[Z_i] = n/N$ for the simple random sample. Intuitively, the probability of being included in the sample is just the fraction of the sample being selected, $n/N$.  

The second salient feature of an estimator's sampling distribution is how spread out it is. Generally speaking, we would prefer an estimator whose estimates are very similar from sample to sample, rather than an estimator whose estimates vary wildly from one sample to the next. We quantify this spread with the **sampling variance** which is just the variance of the sampling distribution of the estimator, or 
$$
\V[\Xbar_n] = \E[(\Xbar_n - \E[\Xbar_n])^2].
$$
An alternative measure of spread is the **standard error** of an estimator, which is just the square root of the sampling variance, 
$$
\se[\Xbar_n] = \sqrt{\V[\Xbar_n]}.
$$
The standard error is often more interpretable because it is on the same scale as the original variable. So, if we were measuring income in the United States, the standard error would be measured in dollars, whereas the sampling variance would be measured in dollars squared. 

The final feature of interest is the **mean squared error** or **MSE**, which (as its name implies) measures the average of the squared error, 
$$
\text{MSE} = \E[(\Xbar_n-\overline{x})^2].
$$
The keen-eyed reader might find this quantity redundant, because we showed above that the sample mean is unbiased so $\E[\Xbar_n] = \overline{x}$, which would mean that the sampling variance of the sample mean is just the mean squared error. However, circumstances will often conspire to have us use biased estimators and so these two quantities will be different. In fact, if we have an estimator $\widehat{\theta}$ for some population quantity $\theta$, 
$$ 
\begin{aligned}
\text{MSE}[\widehat{\theta}] &= \E[(\widehat{\theta} - \theta)^2] \\
&= \E[(\widehat{\theta} - \E[\widehat{\theta}] + \E[\widehat{\theta}] - \theta)^2] \\
&= \E[(\widehat{\theta} - \E[\widehat{\theta}])^2] + \left(\E[\widehat{\theta}] - \theta\right) ^ 2 + 2\E[(\widehat{\theta} - \E[\widehat{\theta}])]\left(\E[\widehat{\theta}] - \theta\right) \\
&= \text{bias}[\widehat{\theta}_n]^2 + \V[\widehat{\theta}_n]  
\end{aligned}
$$ 
We can see, then, that the MSE is low when bias and variance are low. 

One way to conceptualize all of these concepts is to connect them to notions of precision and accuracy. In particular, estimators with low sampling variance are **precise**, whereas estimators with low MSE are **accurate**. We might have an estimator that is very precise, but inaccurate because it is biased. 




## Question 5: Uncertainty

We have a population, a quantity of interest, a sampling design, an estimator, and, with data, an actual estimate. But if we have sampled, say, Sam and Merry from our hobbit population and obtained a sample mean height of 127, we might worry about all of the other samples and sample means we could have drawn. Is our estimate of 127 inches close to the true population mean? We cannot truly know without conducting a complete census, which would render our sampling pointless. Can we instead figure out how far we might be from the truth? The sampling variance actually answers this exact question, but it depends on the sampling distribution and we only have a single draw from this distribution: our estimate of 127. 

If we have a specific estimator and sampling design, we can usually derive an analytical expression for the sampling variance (and, thus, the standard error). This will help us understand what factors will influence the sampling variance. To help us in this endeavor, we need to define an additional feature of the population distribution, the **population variance**,
$$
s^{2}= \frac{1}{N-1} \sum_{i=1}^{N} (x_{i} - \overline{x})^{2}.
$$
The population variance is a fixed quantity, measuring the spread of the $x_i$ values in the population. 

We can now write the sampling variance of $\Xbar_n$ under simple random sampling as
$$
\V[\Xbar_{n}] = \left(1 - \frac{n}{N}\right) \frac{s^{2}}{n}
$$
From this expression, we can determine a few relationships. First, if the the data $x_i$ is more spread out in the population, the sample mean will also be more spread out. Second, the larger the sample size, $n$, the smaller the sampling variance (for a fixed population size). Third, the larger the population size, $N$, the smaller the sampling variance (for a fixed sample size). 

### Deriving the sampling variance of the sample mean

How did we obtain this expression for the sampling variance under simple random sampling? We might be tempted to simply say "someone else proved it for me," but this blind faith in statistical theory limits our own understanding of this situation and our own ability to navigate novel scenarios that arise in our research lives. 

Let's begin with a simple application of the rules of variance that would be valid for any sampling design:
$$
\V[\Xbar_{n}] = \V\left[\frac{1}{n} \sum_{i=1}^N x_iZ_i\right] = \frac{1}{n^2}\left[\sum_{i=1}^N x_i^2\V[Z_i] + \sum_{i=1}^N\sum_{j\neq i} x_ix_j\cov[Z_i,Z_j]\right].
$$
Notice in the second equality how the $x_i$ and $x_j$ values come out of the variance and covariance operators like they are constants. This is because in design-based inference, they are exactly constants. The only source of variation and uncertainty comes from the sampling, symbolized by the inclusion indicators, $Z_i$. Clearly, to make progress, we will need to know the variance and covariance of these inclusion indicators. Recall that the variance of a  binary indicator with probability $p$ of being 1 is $p(1 - p)$. So if we have $\P(Z_i = 1) = n/N$ for a simple random sample, we have 
$$
\V[Z_i] = \frac{n}{N}\left(1 - \frac{n}{N}\right) = \frac{n(N - n)}{N^2}.
$$

If you are used to the "independent and identically distributed" framework (to which we will turn in the next chapter), the covariances in our sampling variances might surprise you. Don't we usually think of our units as independent? While this assumption would (and will) make our math lives easier, it is not true for the simple random sample. The srs samples units without replacement, which implies that inclusion is not independent---knowing that unit $i$ was included the sample means that another unit $j$ only has a $(n-1)/(N-1)$ probability of being included in the sample. To derive an expression for the covariance, note that $\cov(Z_i, Z_j) = \E[Z_iZ_j] - \E[Z_i]\E[Z_j]$ and 
$$
\E[Z_iZ_j] = \P(Z_i = 1, Z_j = 1) = \P(Z_i = 1)\P(Z_j =1 \mid Z_i = 1) = \frac{n}{N}\cdot \frac{n-1}{N-1}.
$$
Plugging this into our covariance statement, we get, 
$$
\begin{aligned}
\cov(Z_i, Z_j) &= \E[Z_iZ_j] - \E[Z_i]\E[Z_j] \\ &= \frac{n}{N}\cdot \frac{n-1}{N-1} - \frac{n^2}{N^2}. \\
&=\frac{n}{N}\left(\frac{n-1}{N-1} - \frac{n}{N}\right) \\
&= \frac{n}{N}\left(\frac{Nn-N - Nn + n}{N(N-1)}\right) \\
&= -\frac{n(N- n)}{N^2(N-1)} \\
&= -\frac{\V[Z_i]}{N-1}.
\end{aligned}
$$
Given that variances and population sizes are positive, we can see that the covariance between the inclusions of two units is negative. Given that there are a fixed number of spots in the sample, Frodo being included lowers the chances that Sam is included, so we end up with this negative covariance. 

With the covariance and variance in hand, we can derive the sampling variance of the sample mean. 
$$
\begin{aligned}
\V[\Xbar_{n}] &=  \frac{1}{n^2}\left[\sum_{i=1}^N x_i^2\V[Z_i] + \sum_{i=1}^N\sum_{j\neq i} x_ix_j\cov[Z_i,Z_j]\right] \\
&=  \frac{1}{n^2}\left[\sum_{i=1}^N x_i^2\V[Z_i] - \frac{1}{N-1} \sum_{i=1}^N\sum_{j\neq i} x_ix_j\V[Z_i]\right] \\
&=  \frac{\V[Z_i]}{n^2}\left[\sum_{i=1}^N x_i^2 - \frac{1}{N-1} \sum_{i=1}^N\sum_{j\neq i} x_ix_j\right] \\
&=  \frac{N-n}{nN^2}\left[\sum_{i=1}^N x_i^2 - \frac{1}{N-1} \sum_{i=1}^N\sum_{j\neq i} x_ix_j\right]
\end{aligned}
$$
Where do we go from here? Unfortunately, we now arrive at the non-obvious and seemingly magical step of "adding and subtracting a crucial quantity." This step appears magical because you seem to have to know the step before you can complete the proof, but how could you ever complete the proof without knowing this step? In this case, we need to add and subtract the quantity $(N-1)^{-1} \sum_{i=1}^N x_i^2$. Why this quantity and how could we ever discover it on our own? Let's rewrite the population variance in a slightly different way to see what we might want to find,
$$
s^2 = \frac{1}{N-1}\sum_{i=1}^N (x_i - \overline{x})^2 = \frac{1}{N-1} \left(\sum_{i=1}^N x_i^2  - N\overline{x}^2\right)
$$
Notice that we can write
$$
N^2\overline{x}^2 = \sum_{i=1}^N x_i^2 + \sum_{i=1}^N \sum_{j\neq i} x_ix_j, 
$$
which provides a hint as to the quantity that we will add and subtract
$$
\begin{aligned}
\V[\Xbar_{n}] &=  \frac{N-n}{nN^2}\left[\sum_{i=1}^N x_i^2  \textcolor{red!50}{\underbrace{+ \frac{1}{N-1}\sum_{i=1}^N x_i^2 - \frac{1}{N-1}\sum_{i=1}^N x_i^2}_{\text{add and subtract}}} - \frac{1}{N-1} \sum_{i=1}^N\sum_{j\neq i} x_ix_j\right] \\
&=  \frac{N-n}{nN^2}\left[\frac{N}{N-1}\sum_{i=1}^N x_i^2  - \frac{1}{N-1} \sum_{i=1}^N\sum_{j=i}^N x_ix_j\right] \\
&=  \frac{N-n}{nN^2}\left[\frac{N}{N-1}\sum_{i=1}^N x_i^2  - \frac{N^2}{N-1} \overline{x}\right] \\
&=  \frac{N-n}{nN(N-1)}\left[\sum_{i=1}^N x_i^2  - N \overline{x}\right] \\
&=  \frac{N-n}{nN(N-1)}\sum_{i=1}^N (x_i - \overline{x})^2 \\
&= \frac{(N-n)}{N}\frac{s^2}{n} = \left(1 - \frac{n}{N}\right)\frac{s^2}{n}.
\end{aligned}
$$
This proof is rather involved, but it does display some commonly-used approaches to deriving statistical results. It also highlights how the sampling scheme leads to dependence that makes the result more complicated. In the next chapter, we will see that the variance of the sample mean under indepdent and identically distributed sampling is much simpler. 

### Estimating the sampling variance

One unfortunate aspect of the sampling variance, $\V[\Xbar_n]$, is that it depends on the population variance, $s^2$, which we cannot know unless have conducted a census of the entire population. Of course, in that case, we would hardly need to worry about uncertainty. Thus, we will need some way to estimate the sampling variance. Since we already know $n$ and $N$, the most straightforward way to do this is to find an estimator for the population variance. 

The **sample variance** is the variance formula applied to the sample itself,
$$
S^2 = \frac{1}{n-1} \sum_{i=1}^N Z_i(x_i - \Xbar_n)^2.
$$
We can obtain an estimator for the sampling variance by plugging this in for the population variance, 
$$
\widehat{\V}[\Xbar_n] = \left(1 - \frac{n}{N}\right)\frac{S^2}{n}.
$$
 


::: {.callout-warning}

## Mind your variances

It is easy to get confused about the difference between the population variance, the sampling variance, and the sample variance. Adding to the confusion is that these are both variances, but for different distributions. 


:::


Is $\widehat{\V}[\Xbar_n]$ a good estimator for $\V[\Xbar_{n}]$? To answer this question, we can apply the same criteria as above in Question 4. Ideally, our estimator would be unbiased, which in this case would be that it does not systematically over- or underestimate how much variation there is in the sample mean across samples. 

$$
\begin{aligned}
\E[S^2] &= \frac{1}{n-1} \sum_{i=1}^N \E[Z_i(x_i - \Xbar_n)^2] \\
&= \frac{1}{n-1}  \E\left[\sum_{i=1}^N Z_i(x_i - \overline{x} - (\Xbar_n - \overline{x}))^2\right] \\
&= \frac{1}{n-1}  \E\left[\sum_{i=1}^N Z_i(x_i - \overline{x})^2 -2Z_i(x_i - \overline{x})(\Xbar_n - \overline{x}) + Z_i(\Xbar_n - \overline{x})^2\right] \\
\end{aligned}
$$

Notice that $(\Xbar_n - \overline{x})$ does not depend on $i$ so we can pull it out of the summations:

$$
\begin{aligned}
\E[S^2] &= \frac{1}{n-1}  \E\left[\sum_{i=1}^N Z_i(x_i - \overline{x})^2 -2(\Xbar_n - \overline{x}) \sum_{i=1}^N Z_i(x_i - \overline{x}) + (\Xbar_n - \overline{x})^2 \sum_{i=1}^N Z_i\right] \\
 &= \frac{1}{n-1}  \E\left[\sum_{i=1}^N Z_i(x_i - \overline{x})^2 -2n(\Xbar_n - \overline{x})^2  + n(\Xbar_n - \overline{x})^2\right] \\
 &= \frac{1}{n-1}  \left[\sum_{i=1}^N \E[Z_i] (x_i - \overline{x})^2 -n\E[(\Xbar_n - \overline{x})^2]\right] \\
 &= \frac{n}{N}\frac{1}{n-1}\sum_{i=1}^N (x_i - \overline{x})^2 -\frac{n}{n-1}\V[\Xbar_n] \\
 &= \frac{n(N-1)}{N(n-1)}s^2 -\frac{(N-n)}{N(n-1)}s^2 \\
 &= s^2
\end{aligned}
$$

We have shown that the sample variance is unbiased for the population variance. To complete everything, we can just plug this into our estimated sampling variance, 
$$
\E\left[\widehat{\V}[\Xbar_n]\right] = \left(1 - \frac{n}{N}\right)\frac{\E\left[S^2\right]}{n} = \left(1 - \frac{n}{N}\right)\frac{s^2}{n} = \V[\Xbar_n],
$$
establishing that our estimator is unbiased. 

## Stratified sampling and survey weights

True to its name, the simple random sample is perhaps the most obvious way to conduct a random sample of a fixed size. If we have more information about the population, however, we might be able to obtain better estimates of our population quantities by incorporating this information into our sampling scheme. We can do this by conducting a **stratified random sample**, where we divide up the population into several strata (or groups) and conduct simple random samples within each stratum. We create these strata (or "stratify the population" in the usual jargon) based on the additional information about the population. 

For a concrete example of stratification, consider an expanded population of the entire fellowship of the ring. 


| Unit ($i$)  | Race   | Height in cm ($x_i$) |
|:------------|:-------|:---------------------|
| 1 (Frodo)   | Hobbit | 124                  |
| 2 (Sam)     | Hobbit | 127                  |
| 3 (Pip)     | Hobbit | 123                  |
| 4 (Merry)   | Hobbit | 127                  |
| 5 (Gimli)   | Dwarf  | 137                  |
| 6 (Gandalf) | Wizard | 168                  |
| 7 (Aragorn) | Man    | 198                  |
| 8 (Boromir) | Man    | 193                  |
| 9 (Legolas) | Elf    | 183                  |

We could conduct stratified sampling here by splitting our population into two strata: hobbits and non-hobbits, making up 4/9ths $\approx$ 44% and 5/9ths $\approx$ 56% of the population, respectively. If we were taking a sample of size 5 from this population, we could use a simple random sample, but we could end up with a very lopsided sample. We might, for instance, sample mostly or all non-hobbits. We can avoid this by conducting simple random samples of size 2 for the hobbits and size 3 for the non-hobbits. This would guarantee our sample would be 40% hobbit every time, while still maintaining randomness in our selection of which hobbits and non-hobbits go into the sample.   

Another reason to conduct a stratified random sample is to guarantee a level of precision for a certain subgroup of the population. In the social sciences, we often conduct nationally representative surveys, but have a specific interest in obtain estimates for certain minority populations like African Americans, Latinos, LGBT people, and others. In modest sample sizes, the number of respondents in one of these groups might be too small to learn much about their opinions. Thus, we might sample a higher proportion of the group of interest to ensure we can make precise statements about that group. 

Stratified random sampling is one example of a broad class of sampling methods that have unequal inclusion probabilities, which we denote $\pi_i = \P(Z_{i} = 1)$. In a simple random sample, we have $\pi_i = n/N$ for all $i$. In the above fellowship example, we were sampling 2 hobbits and 3 non-hobbits, so we have the following inclusion probabilities:


| Unit ($i$)  | Race   | Inclusion probability ($\pi_i$) |
|:------------|:-------|:--------------------------------|
| 1 (Frodo)   | Hobbit | 0.5                             |
| 2 (Sam)     | Hobbit | 0.5                             |
| 3 (Pip)     | Hobbit | 0.5                             |
| 4 (Merry)   | Hobbit | 0.5                             |
| 5 (Gimli)   | Dwarf  | 0.6                             |
| 6 (Gandalf) | Wizard | 0.6                             |
| 7 (Aragorn) | Man    | 0.6                             |
| 8 (Boromir) | Man    | 0.6                             |
| 9 (Legolas) | Elf    | 0.6                             |

There are additional ways that we might conduct a random sample with unequal inclusion probabilities. For example, suppose we were randomly sampling 5 cities to study from the United States. We might want to bias our sample toward larger cities to capture a larger number of citizens in the overall sample. In that case, if the number of inhabitants for city $i$ is $b_i$, then our inclusion probabilities for sampling with replacement[^pps] would be 
$$
\pi_i = \frac{b_i}{\sum_{i=1}^N b_i}.
$$
Here again, we are using information about the population in our sampling design, though this information is more continuous whereas the information in the stratified estimator is discrete. 

[^pps]: This description is true for sampling with replacement. When we want to sample without replacement, we would need to adjust the probabilities to account for how being selected first means that a unit cannot be selected second. 


When using a sampling design with unequal inclusion probabilities, we have obviously changed our sampling design (question 3), but the population and estimands (questions 1 and 2) remain the same. We are still interested in estimating the population mean, $\overline{x}$. We now turn to our estimator (question 4), since we will need to use a new estimator that matches the design. 

There are actually two estimators that are commonly used to estimate the population mean when sampling with unequal inclusion probabilities. The first is called the **Horvitz-Thompson (HT) estimator** and has the form
$$ 
\widetilde{X}_{HT} = \frac{1}{N} \sum_{i=1}^{N} \frac{Z_{i}x_{i}}{\pi_{i}},
$$
where we take the weighted average of those in our sample where the weights are the inverse of the inclusion probabilities. This is why the estimator is sometimes called the inverse probability weighting or IPW estimator. 

We can show that the HT estimator is unbiased for the population mean by noting that $\E[Z_i] = \P(Z_i = 1) = \pi_i$, so that
$$
\E[\widetilde{X}_{HT}] = \frac{1}{N} \sum_{i=1}^N \frac{\E[Z_i]x_i}{\pi_i} = \frac{1}{N} \sum_{i=1}^N x_i = \overline{x}. 
$$
One downside of the HT estimator is that it can be unstable a unit with a very small inclusion probability is selected, since their weight $1/\pi_i$ will be very large. This instability is the cost of being unbiased for the stratified design. We can write the formula for the sampling variance, but it is rather complicated and requires notation that is less important to the task at hand. 

The **Hájek estimator** normalizes the weights so they sum to $N$ and has the form
$$ 
\widetilde{X}_{hj} = \frac{\sum_{i=1}^N Z_{i}x_{i} / \pi_{i}}{\sum_{i=1}^{N} Z_{i}/\pi_{i}}.
$$
This estimator is **biased** for the population mean since there is a random quantity in the denominator. The Hajek estimator is often considered the better estimator in many situations, though, because it have lower sampling variance than the HT estimator. 


### Sampling weights

The HT and Hajek estimators are both functions of what are commonly called the **sampling weights**,
$$w_i = \frac{1}{\pi_i}$$.
We can write the HT estimator as
$$
\widetilde{X}_{HT} = \frac{1}{N} \sum_{i=1}^N w_iZ_ix_i,
$$
and we can write the Hajek estimator as
$$
\widetilde{X}_{hj} = \frac{\sum_{i=1}^N w_iZ_{i}x_{i}}{\sum_{i=1}^{N} w_iZ_{i}}.
$$
These weights, $w_i$, are commonly distributed with surveys because they are the only piece of information from the sampling design that we need to use the above estimators. This allows researchers to analyze a randomly sampled survey without knowledge of the exact design.[^var]

[^var]: If we want design-based estimators of the sampling variance, we would also need to know the joint inclusion probabilities, which are the probabilities of any two units being sampled together. 

The sampling weights have a nice interpretation in terms of a pseudo-population. Each unit in the sample "represents" $w_i = 1/\pi_i$ units in the population, thus making the sample more representative of the population. 

Finally, we note that statistical software often is a little confusing in terms of how it handles weights. It may not be obvious what estimator the R function `weighted.mean(x, w)` is using. In fact, looking at the source code, we find that it basically calls
```{r}
#| eval: false
sum(x * w) / sum(w)
```
which is equivalent to the Hajek estimator above. 


## Conclusion

This chapter covered the basic structure of design-based inference in the context of survey sampling. We introduced the basic questions of statistical inference, including specifying a population and quantity of interest, choosing a sampling design and estimator, and assessing uncertainty of our estimator. Of course, we have only scratched the surface of the types of designs and estimators used in the practice of survey sampling. Professional probability surveys often use clustering, which means randomly selecting larger clusters of units and then randomly sampling within these units. When combined with stratification, surveys end up being very complex. No matter how complex they are, though, the survey weights and associated estimators in the previous section are usually valid ways to estimate population quantities. 
