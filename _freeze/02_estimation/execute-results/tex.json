{
  "hash": "f03666117eff54485c752d5b6f03c856",
  "result": {
    "markdown": "# Estimation\n \n\n## Introduction\n\nWhen studying probability, we assumed that we knew the parameter of a distribution (the mean, the variance, etc), and we used probability to understand what kind of data we would observe. Now we will put this engine in reverse and try to learn about the data generating process or some feature of it using only the observed data that we have. There are two main goals here: **estimation** which is how we formulate our best guess about a parameter of the DGP, and **inference** which is how we formalize and express uncertainty about our estimates. \n\n**Insert two-direction diagram**\n\n\n::: {#exm-rct}\n\n## Randomized control trial\n\nSuppose we are conducting a randomized experiment on framing effects. All respondents receive some factual information about recent levels of immigration, but the message for the treatment group ($D_i = 1$) has an additional framing of the positive benefits of immigration and the control group ($D_i = 0$) receives no additional framing. The outcome is a binary outcome on whether the respondent supports increasing legal immigration limits ($Y_i = 1$)\nor not ($Y_i = 0$). The observed data consists of $n$ pairs of random variables, the outcome and the treatment assignment: $\\{(Y_1, D_1), \\ldots, (Y_n, D_n)\\}$. Define the two sample means/proportions in each group as \n$$\n\\Ybar_1 = \\frac{1}{n_1} \\sum_{i: D_i = 1} Y_i,  \\qquad\\qquad \\Ybar_0 = \\frac{1}{n_0} \\sum_{i: D_i = 0} Y_i,\n$$\nwhere $n_1 = \\sum_{i=1}^n D_i$ is the number of treated units and $n_0 = n - n_1$ is the number of control units. \n\nA common estimator for the treatment effect in a study like this would be the difference in means, $\\Ybar_1 - \\Ybar_0$. But this is only one possible estimator. We could also estimate the effect by taking this difference in means separately by party identification and then averaging those party-specific effects by the size of those groups. This is commonly called a **poststratification** estimator, but it's unclear at first glance which of these two estimators we should prefer. \n\n:::\n\n\nWhat are the goals of studying estimators? In short, we prefer to use **good** estimators rather than **bad** estimators. But what makes an estimator good or bad? You probably have some intuitive sense that, say, an estimator that returns the value 3 is bad, but it will be helpful for us to formally define and explore some properties of estimators that will allow us to compare them and choose the good over the bad. \n\n\n## Samples and populations\n\n\n\n\nFor most of this class, we'll focus on a relatively simple setting where we have a random vectors $\\{X_1, \\ldots, X_n\\}$ that are **independent and identically distributed** (iid) draws from a distribution with cumulative distribution function (cdf) $F$. They are independent in the sense that the random vectors $X_i$ and $X_j$ are independent for all $i \\neq j$, and the they are identically distributed in the sense that each of the random variables $X_i$ have the same marginal distribution, $F$. \n\n\n\nYou can think of each of these vectors, $X_i$, as the rows in your data frame. Note that we're being purposely vague about this cdf---it just represents the unknown distribution of the data, otherwise known as the **data generating process** (DGP). Sometimes $F$ is also referred to as the **population distribution** or even just **population**, which has its roots in viewing the data as a random sample from some larger population. As a shorthand, we often say that the collection of random vectors $\\{X_1, \\ldots, X_n\\}$ is a **random sample** from population $F$ if $\\{X_1, \\ldots, X_n\\}$ is iid with distribution $F$. The **sample size** $n$ is the number of units in the sample. \n\n\nThere are two metaphors that can help build intuition about the concept of viewing the data as an iid draw from $F$:\n\n1. **Random sampling**. Suppose we have a population of size $N$ which is much, much larger than our sample size $n$, and we take a simple random sample of size $n$ from this population. Then the distribution of the data in the random sample will be iid draws from the population distribution of the variables we are sampling. For instance, suppose we take a random sample from a population of US citizens where the population proportion of Democratic party identifiers is 0.33. Then if we randomly sample $n = 100$ US citizens, each data point $X_i$ will be distributed Bernoulli with probability of success 0.33. \n2. **Groundhog Day**. Random sampling does not always make sense as a justification for iid data, especially when the units are not really samples at all, but rather countries, states, or subnational units. In this case, we have to appeal to thought experiment, where $F$ represents the fundamental uncertainty about how the data was generated. The metaphor here is that if we could re-run history many times, like the 1993 American classic comedy *Groundhog Day*, data and outcomes would change slightly due to the inherently stochastic nature of the world. The iid assumption, then, is that each of the units in our data have the same DGP producing this data or the same distribution of outcomes under the *Groundhog Day* scenario.[1] \n\nNote that there are many, many situations where the iid assumption does not make sense. We will cover some of those later in the semester. But much of the innovation and growth in statistics over the last, say, 50 years has been in figuring out how to do statistical inference when iid is violated and often the solutions are specific to the type of iid violation you have (spatial, time-series, network, clustered, etc). As a rule of thumb, though, if you suspect a violation of iid data, your statements of uncertainty will likely be overconfident (for example, confidence intervals, which we'll cover later, are too small). \n\n## Point estimation\n\n### Quantities of interest\n\nOur goal is to learn about the data generating process, represented by the cdf, $F$. We might be interested in estimating the cdf at a general level or we might only be interested in estimating some feature of the distribution, like a mean or conditional expectation function. We will almost always have a particular goal in mind, but it's useful to introduce the idea of estimation in a general way since most of the concepts about estimation, so we'll let $\\theta$ represent the quantity of interest. **Point estimation** is the process of providing a single \"best guess\" about theta whatever quantity of interest we choose, $\\theta$. \n\n::: {.callout-note}\n\nQuantities of interest are also referred to as **parameters** or **estimands** (that is, the target of estimation).\n\n:::\n\n\n::: {#exm-prop}\n\n## Population mean\n\nSuppose we wanted to know the proportion of US citizens who support increasing the level of legal immigration in the US, which we denote as $Y_i = 1$. Then our quantity of interest is the mean of this random variable, $\\mu = \\E[Y_i]$, which is also the probability of randomly drawing someone from the population that supports increasing legal immigration. \n\n:::\n\n::: {#exm-var}\n\n## Population variance\n\nFeeling thermometer scores are a very common way to assess how a survey respondent feels about a particular person or group of people. Respondents are asked how warmly they feel about a group from 0 to 100, which we will denote $Y_i$. We might be interested in how polarized views are on a group in the population and one measure of polarization could the be variance, or spread, of the distribution of $Y_i$ around the mean. In this case, $\\sigma^2 = \\V[Y_i]$ would be our quantity of interest. \n\n:::\n\n\n::: {#exm-rct-ii}\n\n## RCT, continued\n\nIn @exm-rct we discussed a common estimator for an experimental study with a binary treatment. The goal of that experiment is to learn about the difference between two conditional probabilities (or expectations): the average support for increasing legal immigration in the treatment group, $\\mu_1 = \\E[Y_i \\mid D_i = 1]$, and the same average in the control group, $\\mu_0 = \\E[Y_i \\mid D_i = 0]$. That is, we want to know about $\\mu_1 - \\mu_0$, a function of unknown features of these two conditional distributions. \n\n:::\n\nEach of these is a function of the (possibly joint) distribution of the data, $F$. In each of these, we are not necessarily interested in the entire distribution, just summaries of it (central tendency, spread). Of course there are situations where we are also interested in the complete distribution. \n\n### Estimators \n\nWhen our sample size is more than a few observations, it makes no sense to work with the raw data, $X_1, \\ldots, X_n$, and we inevitably will need to *summarize* the data in some way. We can represent this summary as a function, $g(x_1, \\ldots, x_n)$, which might the formula for the sample mean or sample variance. This function is just a regular function that takes in $n$ numbers (or vectors) and returns a number (or vector). We can also define a random variable based on this function, $Y = g(X_1, \\ldots, X_n)$, which inherits its randomness from the randomness of the data: before we see the data, we don't know what values of $X_1, \\ldots, X_n$ we will see and so we don't know what value of $Y$ we'll see either. We call the random variable $Y = g(X_1, \\ldots, X_n)$ a **statistic** (or sometimes sample statistics) and we refer to the probability distribution of a statistic $Y$ as the **sampling distribution** of $Y$. \n\n\n::: {.callout-warning}\n\nThere is one potential confusion in how we talk about \"statistics.\" Just above we defined a statistic as a random variable, based on it being a function of random variables (ie, the data). But we also sometimes refer to the calculated value as a statistic as well, which is a specific number that you see in your R output. To be precise, we should call the latter the **realized value** of the statistic, but message discipline is difficult to enforce in this context. A simple example might help. Suppose that $X_1$ and $X_2$ are the result of a roll of two standard six-sided dice. Then the statistic $Y = X_1 + X_2$ is a random variable that has a distribution over the numbers from \\{2, \\ldots, 12\\} that describes our uncertainty over what the sum will be *before we roll the dice*. Once, we roll the dice and observed the realized values $X_1 = 3$ and $X_2 = 4$, we observed the realized value of the statistic, $Y = 7$. \n\n:::\n\nAt their most basic, statistics are just summaries of the data without aim or ambition. Estimators are statistics with a purpose: to provide an \"educated guess\" about some quantity of interest. \n\n::: {#def-estimator}\nAn **estimator** $\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)$ for some parameter $\\theta$, is a statistic intended as a guess about $\\theta$.\n:::\n\nOne important distinction of jargon is between an estimator and an estimate, similar to issues of statistic above. The estimator is a function of data, whereas the **estimate** is the *realized value* of the estimator once we see the data. An estimate is a single number, such as 0.38, whereas the estimator is a random variable that has uncertainty over what value it will take. Formally, the estimate is $\\theta(x_1, \\ldots, x_n)$ when the data is $\\{X_1, \\ldots, X_n\\} = \\{x_1, \\ldots, x_n\\}$, whereas we represent the estimator as a function of random variables, $\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)$. \n\n::: {.callout-note}\n\nIt is very common, though not universal, to use the \"hat\" notation to define an estimator along with its estimand. For example, $\\widehat{\\theta}$ (or \"theta hat\") indicates that this estimator is targeting the parameter $\\theta$. \n\n:::\n\n::: {#exm-mean-est}\n\n## Estimators for population mean\n\nSuppose we would like to estimate the population mean of $F$, which we will represent as $\\mu = \\E[X_i]$. There are several estimators that we could choose from, all with different properties. \n$$\n\\widehat{\\theta}_{n,1} = \\frac{1}{n} \\sum_{i=1}^n X_i, \\quad \\widehat{\\theta}_{n,2} = X_1, \\quad \\widehat{\\theta}_{n,3} = \\text{max}(X_1,\\ldots,X_n), \\quad \\widehat{\\theta}_{n,4} = 3\n$$\nThe first is just the sample mean, which is an intuitive and natural estimator for the population mean. The second just uses the first observation. While this seems silly, this is a valid statistic (it's a function of the data!). The third just takes the maximum value in the sample, and the fourth always returns 3 no matter what the data says. \n:::\n\n## How to find estimators\n\nWhere do estimators come from? There are a couple of different methods that I'll cover briefly here before describing the ones that will form the bulk of this class. \n\n### Parametric models and maximum likelihood \n\nThe first method for generating estimators is based on **parametric models**, where the researcher specifies the exact distribution (up to some unknown parameters) of the DGP. Let $\\theta$ be the parameters of this distribution and we then write $\\{X_1, \\ldots, X_n\\}$ are iid draws from $F_{\\theta}$. We should also formally state the set of possible values that the parameters can take, which we call the **parameter space** and usually denote as $\\Theta$. Because we're assuming we know the distribution of the data, we can write the p.d.f. as $f(X_i \\mid \\theta)$ and define the likelihood function as the product of these p.d.f.s over the units as a function of the parameters:\n$$\nL(\\theta) = \\prod_{i=1}^n f(X_i \\mid \\theta).\n$$\nWe can then define the **maximum likelihood**  estimator (MLE) for $\\theta$ as the values of the parameter that, well, maximize the likelihood:\n$$\n\\widehat{\\theta}_{mle} = \\argmax_{\\theta \\in \\Theta} \\; L(\\theta)\n$$\nSometimes we can use calculus to derive a closed-form expression for the MLE, but more often we will use iterative techniques that essentially search the parameter space for the maximum. \n\nMaximum likelihood estimators have very nice properties, especially in large samples. Unfortunately, it also requires the correct knowledge of the parametric model, which is often difficult to justify. Do we really know if a given event count variable should be modeling as Poisson or Negative Binomial? The nice properties of MLE are only as good as our ability to get these types of choices correct. \n\n::: {.callout-note}\n\n## No free lunch\n\nOne really important intuition to build about statistics is the **assumptions-precision tradeoff**. You can usually get more precise estimates if you make stronger and potentially more fragile assumptions. Conversely, if you want to weaken your assumptions, you will almost always get less precise estimates.\n\n:::\n\n### Plug-in estimators\n\nThe second broad class of estimators are **semiparametric** in the sense that we will specify some finite-dimensional parameters of the DGP, but leave the rest of the distribution unspecified. For example, we might specify a population mean, $\\mu = \\E[X_i]$, and a population variance, $\\sigma^2 = \\V[X_i]$ but leave unrestricted the shape of the distribution. This is really important because our estimators will not be as dependent on correctly specifying distributions that maybe have no business specifying.  \n\nThe basic method for constructing estimators in this setting is to use the **plug-in estimator**, or the estimator that replaces any population mean with a sample mean. Obviously in the case of estimating the population mean, $\\mu$, this means we will use the **sample mean** as its estimate:\n$$\n\\Xbar_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\quad \\text{estimates} \\quad \\E[X_i] = \\int_{\\mathcal{X}} x f(x)dx\n$$\nWhat are we doing here? We are replacing the unknown population distribution $f(x)$ that's in the population mean with a discrete uniform distribution over our data points, with $1/n$ probability assigned to each unit. Why do this? It encodes that if we have a random sample, our best guess about the population distribution of $X_i$ is the sample distribution in our actual data. If this intuition fails, it is also fine to hold onto an analog principle: sample means of random variables are natural estimators of population means. \n\nWhat about estimating something more complicated like a the expected value of a function of the data, $\\theta = \\E[r(X_i)]$? The key is to see that $f(X_i)$ is also a random variable. Let's call this random variable $Y_i = f(X_i)$ now we can see that $\\theta$ is just the population expectation of this random variable and using the plug-in estimator, we get:\n$$\n\\widehat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\frac{1}{n} \\sum_{i=1}^n r(X_i). \n$$\n\nWith these facts in hand, we can describe the more general plug-in estimator. When we want to estimate some quantity of interest that is a function of population means, we can generate a plug-in estimator by replacing any population mean with a sample mean. Formally, let $\\alpha = g\\left(\\E[r(X_i)]\\right)$ be a parameter that is defined as a function of the population mean of a (possibly vector-valued) function of the data. Then, we can estimate this parameter by plugging in the sample mean for the population mean to get the **plug-in estimator**,\n$$\n\\widehat{\\alpha} = g\\left( \\frac{1}{n} \\sum_{i=1}^n r(X_i) \\right) \\quad \\text{estimates} \\quad \\alpha = g\\left(\\E[r(X_i)]\\right)\n$$\nThis approach to plug-in estimation with sample means is very general and will allow us to derive estimators in a large variety of settings. \n\n::: {#exm-var-est}\n\n## Estimating population variance\n\nThe population variance of a random variable is $\\sigma^2 = \\E[(X_i - \\E[X_i])^2]$. To derive a plug-in estimator for this quantity, we replace the inner $\\E[X_i]$ with $\\Xbar_n$ and the outer expectation with another sample mean:\n$$\n\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)^2.\n$$\nNote that this plug-in estimator is not the same as the usual sample variance, which divides by $n - 1$ rather than $n$, but this is a very minor difference that does not matter in moderate to large samples.\n:::\n\n::: {#exm-cov-est}\n\n## Estimating population covariance\n\nSuppose we have two variables, $(X_i, Y_i)$. A natural quantity of interest here is the population covariance between these variables, \n$$\n\\sigma_{xy} = \\text{Cov}[X_i,Y_i] = \\E[(X_i - \\E[X_i])(Y_i-\\E[Y_i])],\n$$\nwhich has the plug-in estimator,\n$$\n\\widehat{\\sigma}_{xy} = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)(Y_i - \\Ybar_n).\n$$\n:::\n\n::: {.callout-note}\n\n## Notation alert\n\nGiven the connection between the population mean and the sample mean, you will sometimes see the $\\E_n[\\cdot]$ operator used as a short hand for the sample average:\n$$\n\\E_n[r(X_i)] \\equiv \\frac{1}{n} \\sum_{i=1}^n r(X_i).\n$$\n\n:::\n\nFinally, we are describing the plug-in estimator only considering replacing population means with sample means, but the idea of plug-in estimation is actually much more broad. We can derive estimators of the population quantiles like the median with sample versions of those quantities. What unifies all of these approaches is replacing the unknown population cdf, $F$, with the empirical cdf, \n$$\n\\widehat{F}_n(x) = \\frac{\\sum_{i=1}^n \\mathbb{I}(X_i \\leq x)}{n}.\n$$\nFor a more complete and technical treatment of these ideas, see Wasserman (2004) Chapter 7.\n\n\n\n## The three distributions: population, empirical, and sampling\n\nOnce we start to wade into estimation, there are several distributions to keep track of and things can quickly become confusing. There are three distributions that are all related and easy to confuse but it's important to keep them distinct. \n\nThe **population distribution** is the distribution of the random variable, $X_i$, which we have labeled $F$. This is the distribution that we want to learn about. Then there is the **empirical distribution**, which is the distribution of the actual realizations of the random variables in our samples (that is, the numbers in our data frame), $X_1, \\ldots, X_n$. Because this is a random sample from the population distribution and can serve as estimator of $F$, we sometimes call this $\\widehat{F}_n$. \n\n\n\n **Insert Sampling distribution figure here**\n\n\nSeparately from both of these is the **sampling distribution of an estimator**, which is the probability distribution of $\\widehat{\\theta}_n$. It represents our uncertainty about what our estimate will be before we see the data. Remember that our estimator is itself a random variable because it is a function of random variables: the data itself. That is, we defined the estimator as $\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)$. \n\n::: {#exm-three-dist}\n## Likert resposes\n\nSuppose $X_i$ is the answer to a question, \"How much do you agree with the following statement: Immigrants are a net positive for the United States,\" with a $X_i = 0$ being \"strongly disagree,\" $X_i = 1$ being \"disgree\", $X_i = 2$ being \"neither agree nor disagree\", $X_i = 3$ being \"agree\", and $X_i = 4$ being \"strongly agree.\"\n\nThe population distribution describes the probability of randomly selecting a person with each one of these values, $\\P(X_i = x)$. The empirical distribution would be the actual numeric values 0-4 in our data. And the sampling distribution of the sample mean, $\\Xbar_n$, would be the distribution of the sample mean across repeated samples from the population. \n\nSuppose that the population distribution was binomial with 4 trials and probability of success $p = 0.4$. We could generate one sample with $n = 10$ and thus one empirical distribution using `rbinom`:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_samp <- rbinom(n = 10, size = 4, prob = 0.4)\nmy_samp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 2 1 3 3 0 2 3 2 1\n```\n:::\n\n```{.r .cell-code}\ntable(my_samp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmy_samp\n0 1 2 3 \n1 3 3 3 \n```\n:::\n:::\n\n\n\nAnd we can generate one draw from the sampling distribution of $\\Xbar_n$ by taking the mean of this sample:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(my_samp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.8\n```\n:::\n:::\n\n\n \nBut obviously if we had a different sample, it would have a different empirical distribution and thus give us a different estimate of the sample mean:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_samp2 <- rbinom(n = 10, size = 4, prob = 0.4)\nmean(my_samp2) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.6\n```\n:::\n:::\n\n\n\nThe sampling distribution is the distribution of these sample means across repeated sampling.\n \n\n:::\n\n\n## Finite-sample properties of estimators\n\nAs we discussed when we introduced estimators, their usefulness is tied to how well they help us learn about the quantity of interest. If we get an estimate $\\widehat{\\theta} = 1.6$, we would like to know that this is \"close\" to the true parameter $\\theta$. The key to understanding how the behavior of our estimators is the sampling distribution. Intuitively, we would like the sampling distribution of $\\widehat{\\theta}_n$ to be as tightly clustered around the true as $\\theta$ as possible. Here, though, we run into a problem: the sampling distribution depends on the population distribution since it is about repeated samples of the data from that distribution filtered through the function $\\theta()$. Since $F$ is unknown, this implies that the sampling distribution will also usually be unknown. \n\nEven though we cannot precisely pin down the entire sampling distribution, we will be able to use assumptions to derive certain properties of the sampling distribution that will be useful in comparing estimators. \n\n\n### Bias\n\nThe first property of the sampling distribution concerns its central tendency. In particular, we will define the  **bias** (or **estimation bias**) of estimator $\\widehat{\\theta}$ for parameter $\\theta$ as\n$$\n\\text{bias}[\\widehat{\\theta}] = \\E[\\widehat{\\theta}] - \\theta. \n$$\nThis is the difference between the mean of the estimator (across repeated samples) and the true parameter. All else equal, we would like estimation bias to be as small as possible. The smallest possible bias, obviously, is 0 and we define an **unbiased estimator** as one with with $\\text{bias}[\\widehat{\\theta}] = 0$ or equivalently, $\\E[\\widehat{\\theta}] = \\theta$. \n\nHowever, all else is not always equal and unbiasedness is not a property to become overly attached to. There are many biased estimators that have other attractive properties and many popular modern estimators are biaseed. \n\n::: {#exm-mean-unbiased}\n\n## Unbiasedness of the sample mean\nWe can show that the sample mean is unbiased for the population mean when the data is iid and $\\E|X| < \\infty$. In particular, we simply apply the rules of expectations:\n$$\\begin{aligned}\n\\E\\left[ \\Xbar_n \\right] &= \\E\\left[\\frac{1}{n} \\sum_{i=1}^n X_i\\right] & (\\text{definition of } \\Xbar_n) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\E[X_i] & (\\text{linearity of } \\E)\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu & (X_i \\text{ identically distributed})\\\\\n&= \\mu.\n\\end{aligned}$$\nNotice that we only used the \"identically distributed\" part of iid. Independence is not needed. \n\n:::\n\n::: {.callout-warning}\n\nProperties like unbiasedness might only hold for a subset of DGPs. For example, we just showed that the sample mean is unbiased, but only when the population mean is finite. There are probability distribution like the Cauchy where the expected value diverges and is not finite. So we are dealing with a restricted class of DGPs that rules out such distributions. You may see this sometimes formalized by defining a class $\\mathcal{F}$ of distributions and unbiasedness might hold in that class if it is unbiased for all $F \\in \\mathcal{F}$. \n\n:::\n\n\n### Estimation variance and the standard error\n\nIf a \"good\" estimator tends to be close to the truth, then we should also care how spread out the sampling distribution is. In particular, we define the **sampling variance** as the variance of an estimator's sampling distribution, $\\V[\\widehat{\\theta}]$. This measures how spread the estimator it is around its mean. For an unbiased estimator, smaller sampling variance implies  the distribution of $\\widehat{\\theta}$ is more concentrated around the truth. \n\n\n::: {#exm-mean-var}\n\n## Sampling variance of the sample mean:\n\nWe can establish the sampling variance of the sample mean of iid data for all $F$ such that $\\V[X_i]$ is finite (more precisely, $\\E[X_i^2] < \\infty$)\n\n$$\\begin{aligned}\n  \\V\\left[ \\Xbar_n \\right] &= \\V\\left[ \\frac{1}{n} \\sum_{i=1}^n X_i \\right] & (\\text{definition of } \\Xbar_n) \\\\\n                           &=\\frac{1}{n^2} \\V\\left[ \\sum_{i=1}^n X_i \\right] & (\\text{property of } \\V)\\\\\n                           &=\\frac{1}{n^2} \\sum_{i=1}^n \\V[X_i] & (\\text{independence})\\\\\n                           &= \\frac{1}{n^2}\\sum_{i=1}^n \\sigma^2 & (X_i \\text{ identically distributed})\\\\\n                           &= \\frac{\\sigma^2}{n}\n\\end{aligned}$$\n\n:::\n\nAn alternative measure of spread for any distribution is the standard deviation, which is on the same scale as the original random variable. We call the standard deviation of the sampling distribution of $\\widehat{\\theta}$ the **standand error** of $\\widehat{\\theta}$:  $\\se(\\widehat{\\theta}) = \\sqrt{\\V[\\widehat{\\theta}]}$. \n\nGiven the above derivation the standard error of the sample mean under iid sampling is $\\sigma / \\sqrt{n}$\n\n\n### Mean squared error\n\nBias and sampling variance clearly got at two different aspects of being a \"good\" estimator. Ideally, we want the estimator to be as close as possible to the true value. One summary measure of the quality of an estimator is the **mean squared error** or **MSE** which is defined as. \n$$\n\\text{MSE} = \\E[(\\widehat{\\theta}_n-\\theta)^2]\n$$\nIdeally, we would have this be as small as possible!\n\nWe can also relate the MSE to the bias and the sampling variance (provided it is finite) with the following decomposition result: \n$$\n\\text{MSE} = \\text{bias}[\\widehat{\\theta}_n]^2 + \\V[\\widehat{\\theta}_n]\n$$\nThis decomposition implies that, for unbiased estimators, MSE is the sampling variance. It also highlights why we might accept some bias for large reductions in variance for lower overall MSE. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Two sampling distributions](02_estimation_files/figure-pdf/mse-1.pdf)\n:::\n:::\n\n\n\nIn this figure, we show the sampling distributions of two estimators, $\\widehat{\\theta}_a$, which is unbiased (centered on the true value $\\theta$) but with a high sampling variance,  and $\\widehat{\\theta}_b$ which is slightly biased but with much lower sampling variance. Even though $\\widehat{\\theta}_b$ is biased, the probability of drawing a value close to the truth is higher than for $\\widehat{\\theta}_a$. This type of balancing between bias and variance is exactly what the MSE helps capture and indeed, in this case, $MSE[\\widehat{\\theta}_b] < MSE[\\widehat{\\theta}_a]$. \n",
    "supporting": [
      "02_estimation_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}