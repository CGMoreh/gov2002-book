{
  "hash": "f042d459f35d612d21d9f62663271ac1",
  "result": {
    "markdown": "# Linear regression\n\n\nRegression refers to a collection of tools for assessing the relationship between a **outcome variable**, $Y_i$, and a set of **covariates**, $\\X_i$. In particular, these tools focus on showing how the conditional mean of $Y_i$ varies as a function $\\X_i$. For example, we may want to know how wait voting poll wait times vary as a function of some socioeconomic features of the precinct like income and racial composition. This is usually accomplished by estimating the **regression function** or **conditional expectation function** (CEF) of the outcome given the covariates, \n$$\n\\mu(\\bfx) = \\E[Y_i \\mid \\X_i = \\bfx].\n$$\nWhy is estimation and inference for this regression function special? Why can't we just use the approaches we have seen for the mean, variance, covariance, and so on? The basic problem with the CEF is that there may be many, many values $\\bfx$ that can occur and so many, many different conditional expectations that we will need to estimate. In fact, if any variable in $\\X_i$ is continuous, then there will be an infinite number of possible values of $\\bfx$ that we need to estimate. Because this problem gets worse as we add covariates to $\\X_i$, this is sometimes referred to as the **curse of dimensionality**. How can we resolve this with our measely finite data?\n\nIn this chapter, we are going to explore two different ways of \"solving\" the curse of dimensionality: assuming it away and changing the quantity of interest to something easier to estimate. \n\n\nRegression is so ubiquitous in so many scientific fields that it has a lot of acquired notational baggage. In particular, the labels of the $Y_i$ and $\\X_i$ varies greatly:\n\n- The outcome can also be called: the response variable, the dependent variable, the labels (in machine learning), the left-hand side variable, or the regressand. \n- The covariates are also called: the explanatory variables, the independent variables, the predictors, the regressors, inputs, or features.  \n\n\n## Why do we need models?\n\nAt first glance, the connection between the CEF and parametric models might be hazy. For example, imagine we are interested in estimating the average poll wait times ($Y_i$) for Black voters ($X_i = 1$) versus non-Black voters ($X_i=0$). In that case, there are two parameters to estimate, \n$$\n\\mu(1) = \\E[Y_i \\mid X_i = 1] \\quad \\text{and}\\quad \\mu(0) = \\E[Y_i \\mid X_i = 0],\n$$\nwhich we could estimate by using the plug-in estimators that replace the population averages with their sample counterparts,\n$$ \n\\widehat{\\mu}(1) = \\frac{\\sum_{i=1}^{n} Y_{i}\\mathbb{1}(X_{i} = 1)}{\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 1)} \\qquad \\widehat{\\mu}(0) = \\frac{\\sum_{i=1}^{n} Y_{i}\\mathbb{1}(X_{i} = 0)}{\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 0)}.\n$$\nThese are just the sample averages of the wait times for Black and non-Black voters, respectively. And because the race variable here is discrete, this basically mimics the situation of estimating a single mean, just within subpopulations defined by race in this case. The same logic would apply if we had $k$ racial categories: we would have $k$ conditional expectations to estimate and $k$ (conditional) sample means.  \n\nNow imagine that we want to know how the average poll wait time varies as a function of income, so that $X_i$ is (essentially) continuous. Now we have a different conditional expectation for every possible dollar amount from 0 to Bill Gates's income. Imagine we pick particular income, \\$42,238, and so we are interested in the conditional expectation $\\mu(42,238)= \\E[Y_{i}\\mid X_{i} = 42,238]$. We could use the same plug-in estimator in the discrete case, \n$$\n\\widehat{\\mu}(42,238) = \\frac{\\sum_{i=1}^{n} Y_{i}\\mathbb{1}(X_{i} = 42,238)}{\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 42,238)}.\n$$\nWhat is the problem with this estimator? In all likelihood there are 0 units in any particular dataset that have that exact income, meaning this estimator is undefined (we would be dividing by zero). \n\n\nOne solution to this problem is to use **subclassification** and turn the continuous variable into a discrete one and proceed with the discrete approach above. We might group incomes into \\$25,000 bins and then calculate the average wait times of anyone between, say, \\$25,000 and \\$50,000 income. When we make this estimator switch for pragmatic purposes, we need to connect it back to the DGP of interest somehow. We could **assume** that the CEF of interest only depends on these binned means, which would mean we have:  \n$$\n\\mu(x) = \n\\begin{cases}\n  \\E[Y_{i} \\mid 0 \\leq X_{i} < 25,000]  &\\text{if }  0 \\leq x < 25,000 \\\\\n  \\E[Y_{i} \\mid 25,000 \\leq X_{i} < 50,000]  &\\text{if }  25,000 \\leq x < 50,000\\\\\n  \\E[Y_{i} \\mid 50,000 \\leq X_{i} < 100,000]  &\\text{if }  50,000 \\leq x < 100,000\\\\\n  \\vdots \\\\\n  \\E[Y_{i} \\mid 200,000 \\leq X_{i}]  &\\text{if }  200,000 \\leq x\\\\\n\\end{cases}\n$$\nThis assumes, perhaps incorrectly, that the average wait time does not vary within the bins. @fig-cef-binned shows a hypothetical joint distribution between income and wait times with the true CEF, $\\mu(x)$ shown in red. The figure also shows the bins created by subclassification and the implied CEF if we assume bin-constant means in blue. We can see that blue function approximates the true CEF but deviates from it especially close to the bin edges. The trade off is that once we make the assumption, we only have to estimate one mean for every bin, rather than an infinite number of means for each possible income. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Hypothetical joint distribution of income and poll wait times (contour plot), conditional expectation function (red), and the conditional expectation of the binned income (blue).](06_linear_model_files/figure-html/fig-cef-binned-1.png){#fig-cef-binned width=672}\n:::\n:::\n\n\nSimilarly, we could **assume** that the CEF follows a simple functional form like a line,\n$$ \n\\mu(x) = \\E[Y_{i}\\mid X_{i} = x] = \\beta_{0} + \\beta_{1} x.\n$$\nThis reduces our infinite number of unknowns (the conditional mean at every possible income) to just two unknowns, the slope and intercept. As we will see, we can use the standard ordinary least squares to estimate these parameters. Notice again, though, that if the true CEF is nonlinear this assumption is incorrect, any estimate based off this assumption might be biased or even inconsistent.  \n\nWe call the binning and linear assumptions on $\\mu(x)$ **functional form** assumptions because \n\n## Regressions with continuous covariates\n\n## Linear CEFs\n\n\n## Linear predictor \n",
    "supporting": [
      "06_linear_model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}